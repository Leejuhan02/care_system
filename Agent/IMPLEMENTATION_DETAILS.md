# ë³´ì¶© ìë£Œ ë° êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

## 1. MoveNet ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜ ê°€ì´ë“œ

### ì˜µì…˜ 1: Kaggleì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
```bash
# Kaggle CLI ì„¤ì¹˜
pip install kaggle

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
kaggle models instances versions google/movenet/1
```

### ì˜µì…˜ 2: TensorFlow Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
```python
# download_movenet.py
import tensorflow_hub as hub
import tensorflow as tf

# MoveNet ëª¨ë¸ URL
model_url = "https://tfhub.dev/google/movenet/singlepose/lightning/4"

# ëª¨ë¸ ë¡œë“œ (ìë™ ë‹¤ìš´ë¡œë“œ)
model = hub.load(model_url)

# TFLiteë¡œ ë³€í™˜
converter = tf.lite.TFLiteConverter.from_saved_model("movenet_model")
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
tflite_model = converter.convert()

# ì €ì¥
with open('./models/movenet_singlepose_lightning.tflite', 'wb') as f:
    f.write(tflite_model)

print("âœ… MoveNet TFLite ë³€í™˜ ì™„ë£Œ!")
```

### ì˜µì…˜ 3: Google Coral ìµœì í™” ë²„ì „
```bash
# Coral ë³´ë“œìš© ìµœì í™”ëœ MoveNet (RPi í˜¸í™˜)
# https://github.com/google-coral/examples-camera/tree/master/ml/pose_estimation
```

---

## 2. ì˜¤ë””ì˜¤ ëª¨ë¸ ì¬í•™ìŠµ ì™„ì „ ê°€ì´ë“œ

### ë‹¨ê³„ 1: í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘

#### ë°ì´í„°ì…‹ êµ¬ì¡°
```
audio_data/
â”œâ”€â”€ scream/              # ë¹„ëª… (600ê°œ ìƒ˜í”Œ)
â”‚   â”œâ”€â”€ 001.wav (1ì´ˆ, 16kHz)
â”‚   â”œâ”€â”€ 002.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ help/                # ë„ì›€ ì‹ í˜¸ (400ê°œ ìƒ˜í”Œ)
â”‚   â”œâ”€â”€ 001.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ words/               # íŠ¹ì • ë‹¨ì–´ (ë„ì›€, 119 ë“±)
â”‚   â”œâ”€â”€ help_001.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ background/          # ê±°ì§“ ì–‘ì„± ë°©ì§€ (900ê°œ)
    â”œâ”€â”€ traffic.wav
    â”œâ”€â”€ music.wav
    â”œâ”€â”€ speech.wav
    â””â”€â”€ ...
```

#### ë°ì´í„° íŠ¹ì„± (ì¤‘ìš”!)
- **ìƒ˜í”Œë§ ë ˆì´íŠ¸**: 16kHz
- **ë¹„íŠ¸ ê¹Šì´**: 16-bit PCM
- **ì§€ì† ì‹œê°„**: ì •í™•íˆ 1ì´ˆ
- **í˜•ì‹**: WAV íŒŒì¼

#### ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬
```python
# record_training_audio.py
import sounddevice as sd
import scipy.io.wavfile as wavfile
import os

sample_rate = 16000
duration = 1  # 1ì´ˆ

def record_sample(category, number):
    print(f"Recording {category} sample {number}...")
    audio = sd.rec(int(sample_rate * duration), samplerate=sample_rate, channels=1, dtype='int16')
    sd.wait()
    
    os.makedirs(f'audio_data/{category}', exist_ok=True)
    filename = f'audio_data/{category}/{number:03d}.wav'
    wavfile.write(filename, sample_rate, audio)
    print(f"Saved: {filename}")

# ì‚¬ìš© ì˜ˆ
# for i in range(1, 21):
#     record_sample('scream', i)
```

### ë‹¨ê³„ 2: ë°ì´í„° ì „ì²˜ë¦¬

```python
# preprocess_audio.py
import numpy as np
import librosa
import os
from scipy.io import wavfile

def preprocess_audio_file(filename, target_sr=16000, duration=1.0):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬"""
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ
    y, sr = librosa.load(filename, sr=target_sr, duration=duration)
    
    # ì •í™•íˆ 1ì´ˆë¡œ íŒ¨ë”©/ìë¥´ê¸°
    target_samples = int(target_sr * duration)
    if len(y) < target_samples:
        y = np.pad(y, (0, target_samples - len(y)), mode='constant')
    else:
        y = y[:target_samples]
    
    # ì •ê·œí™”
    y = y / (np.abs(y).max() + 1e-8)
    
    # MFCC íŠ¹ì„± ì¶”ì¶œ (ì„ íƒì‚¬í•­ - ë” ë‚˜ì€ ëª¨ë¸ ì„±ëŠ¥)
    # mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=13)
    
    return y.reshape(-1, 1)  # [16000, 1] í˜•íƒœ

def prepare_dataset(data_dir='audio_data'):
    """ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„"""
    X = []
    y = []
    
    label_map = {'scream': 0, 'help': 1, 'words': 2, 'background': 3}
    
    for category, label in label_map.items():
        category_path = os.path.join(data_dir, category)
        if not os.path.exists(category_path):
            continue
        
        for filename in os.listdir(category_path):
            if filename.endswith('.wav'):
                filepath = os.path.join(category_path, filename)
                try:
                    audio = preprocess_audio_file(filepath)
                    X.append(audio)
                    y.append(label)
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")
    
    return np.array(X), np.array(y)

# ì‚¬ìš©
# X, y = prepare_dataset()
# print(f"Dataset shape: {X.shape}, Labels: {np.unique(y)}")
```

### ë‹¨ê³„ 3: ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ

```python
# train_audio_model.py
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

def build_audio_model(input_shape=(16000, 1)):
    """ì˜¤ë””ì˜¤ ë¶„ë¥˜ ì‹ ê²½ë§"""
    model = keras.Sequential([
        # Input
        keras.layers.Input(shape=input_shape),
        
        # Conv1D ë¸”ë¡ 1
        keras.layers.Conv1D(64, 80, strides=4, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPooling1D(pool_size=4),
        keras.layers.Dropout(0.3),
        
        # Conv1D ë¸”ë¡ 2
        keras.layers.Conv1D(128, 3, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPooling1D(pool_size=4),
        keras.layers.Dropout(0.3),
        
        # Conv1D ë¸”ë¡ 3
        keras.layers.Conv1D(256, 3, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPooling1D(pool_size=4),
        keras.layers.Dropout(0.3),
        
        # Global pooling
        keras.layers.GlobalAveragePooling1D(),
        
        # Dense ë¸”ë¡
        keras.layers.Dense(256, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dropout(0.5),
        
        # Output (4ê°œ í´ë˜ìŠ¤: scream, help, words, background)
        keras.layers.Dense(4, activation='softmax')
    ])
    
    return model

def train_model(X, y):
    """ëª¨ë¸ í•™ìŠµ"""
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train_flat = X_train.reshape(X_train.shape[0], -1)
    X_test_flat = X_test.reshape(X_test.shape[0], -1)
    X_train_flat = scaler.fit_transform(X_train_flat)
    X_test_flat = scaler.transform(X_test_flat)
    X_train = X_train_flat.reshape(X_train.shape)
    X_test = X_test_flat.reshape(X_test.shape)
    
    # ëª¨ë¸ ë¹Œë“œ
    model = build_audio_model()
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print(model.summary())
    
    # í•™ìŠµ
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=50,
        batch_size=32,
        callbacks=[
            keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
        ]
    )
    
    # í‰ê°€
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {test_acc:.4f}")
    
    return model, history

# ì‚¬ìš© ì˜ˆ
# from preprocess_audio import prepare_dataset
# X, y = prepare_dataset()
# model, history = train_model(X, y)
```

### ë‹¨ê³„ 4: TFLite ë³€í™˜ ë° ìµœì í™”

```python
# convert_to_tflite.py
import tensorflow as tf
import numpy as np

def convert_to_tflite(keras_model, output_path='models/keyword_audio.tflite'):
    """Keras ëª¨ë¸ì„ TFLiteë¡œ ë³€í™˜ ë° ìµœì í™”"""
    
    # ê¸°ë³¸ ë³€í™˜
    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    
    # ìµœì í™” ì˜µì…˜ (RPi 5 í™˜ê²½)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # ì–‘ìí™” (ëª¨ë¸ í¬ê¸° 75% ê°ì†Œ, ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS
    ]
    
    # ë³€í™˜
    tflite_model = converter.convert()
    
    # ì €ì¥
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = len(tflite_model) / (1024 * 1024)
    print(f"âœ… TFLite ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
    print(f"   ê²½ë¡œ: {output_path}")
    print(f"   í¬ê¸°: {file_size_mb:.2f} MB")
    
    return tflite_model

# ì‚¬ìš©
# tflite_model = convert_to_tflite(model)
```

### ë‹¨ê³„ 5: TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸

```python
# test_audio_tflite.py
from tflite_runtime.interpreter import Interpreter
import numpy as np

def test_tflite_model(model_path='models/keyword_audio.tflite', audio_file='test.wav'):
    """TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    # ì¸í„°í”„ë¦¬í„° ë¡œë“œ
    interpreter = Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print(f"ì…ë ¥ shape: {input_details[0]['shape']}")
    print(f"ì¶œë ¥ shape: {output_details[0]['shape']}")
    
    # í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ë¡œë“œ
    import librosa
    y, sr = librosa.load(audio_file, sr=16000, duration=1.0)
    y = (y / (np.abs(y).max() + 1e-8)).reshape(1, -1, 1).astype(np.float32)
    
    # ì¶”ë¡ 
    interpreter.set_tensor(input_details[0]['index'], y)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[0]['index'])
    
    # ê²°ê³¼ í•´ì„
    label_names = ['Scream', 'Help', 'Words', 'Background']
    class_idx = np.argmax(output[0])
    confidence = output[0][class_idx]
    
    print(f"\nê²°ê³¼:")
    print(f"  í´ë˜ìŠ¤: {label_names[class_idx]}")
    print(f"  ì‹ ë¢°ë„: {confidence:.4f}")
    print(f"  ì „ì²´ ì¶œë ¥: {output[0]}")
    
    return output[0]

# ì‚¬ìš©
# test_tflite_model()
```

---

## 3. RPi 5 ì„±ëŠ¥ ìµœì í™”

### A. í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì„¤ì •
```python
# main.py ìˆ˜ì •
import os

def set_process_priority():
    """í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ìƒí–¥"""
    try:
        os.nice(-10)  # ë†’ì€ ìš°ì„ ìˆœìœ„ (-20 ~ 19, ë‚®ì„ìˆ˜ë¡ ë†’ìŒ)
        print("[Main] í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ìƒí–¥ ì„¤ì •")
    except:
        print("[Main] ìš°ì„ ìˆœìœ„ ì„¤ì • ì‹¤íŒ¨ (sudo í•„ìš”í•  ìˆ˜ ìˆìŒ)")

if __name__ == "__main__":
    set_process_priority()
    main()
```

### B. GPU ê°€ì†í™” (TensorFlow Lite Delegate)
```python
# processors.py ìˆ˜ì •
from tflite_runtime.interpreter import Interpreter, load_delegate

# GPU delegate ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
try:
    gpu_delegate = load_delegate('/usr/lib/libGPUDelegate.so')
    interpreter = Interpreter(
        model_path=config.MOVENET_MODEL_PATH,
        experimental_delegates=[gpu_delegate]
    )
    print("[Video] GPU ê°€ì† í™œì„±í™”")
except:
    interpreter = Interpreter(model_path=config.MOVENET_MODEL_PATH)
    print("[Video] CPU ëª¨ë“œ ì‹¤í–‰")
```

### C. ë©”ëª¨ë¦¬ ìµœì í™”
```python
# RPi 5 ì‹œìŠ¤í…œ ì„¤ì •
# /boot/firmware/cmdline.txt ì— ì¶”ê°€:
# cgroup_enable=memory swapaccount=1
```

---

## 4. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (ì„ íƒì‚¬í•­)

ë‚™ìƒ ê°ì§€ ê¸°ë¡ì„ ì €ì¥í•˜ë ¤ë©´:

```python
# db_manager.py
import sqlite3
from datetime import datetime

class FallEventLogger:
    def __init__(self, db_path='fall_events.db'):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        c.execute('''CREATE TABLE IF NOT EXISTS fall_events (
            id INTEGER PRIMARY KEY,
            timestamp TEXT,
            fall_detected BOOLEAN,
            keypoints_quality REAL,
            state TEXT
        )''')
        conn.commit()
        conn.close()
    
    def log_event(self, fall_detected, quality, state):
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        c.execute('''INSERT INTO fall_events 
                     (timestamp, fall_detected, keypoints_quality, state)
                     VALUES (?, ?, ?, ?)''',
                  (datetime.now().isoformat(), fall_detected, quality, state))
        conn.commit()
        conn.close()
```

---

## 5. í´ë¼ìš°ë“œ ì—°ë™ (ì„ íƒì‚¬í•­)

ì•Œë¦¼ ë° ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤:

```python
# cloud_notifier.py
import requests
import json

class CloudNotifier:
    def __init__(self, api_endpoint, api_key):
        self.endpoint = api_endpoint
        self.api_key = api_key
    
    def send_alert(self, message, severity='HIGH'):
        payload = {
            'message': message,
            'severity': severity,
            'timestamp': datetime.now().isoformat()
        }
        
        headers = {'Authorization': f'Bearer {self.api_key}'}
        
        try:
            response = requests.post(
                self.endpoint,
                json=payload,
                headers=headers,
                timeout=5
            )
            return response.status_code == 200
        except Exception as e:
            print(f"Cloud notification failed: {e}")
            return False
```

---

## 6. ë¬¸ì œ í•´ê²° ë° ë””ë²„ê¹…

### ë¡œê·¸ ë ˆë²¨ ì„¤ì •
```python
# ìƒì„¸ ë¡œê¹… ì¶”ê°€ (config.py)
LOG_LEVEL = 'DEBUG'  # DEBUG, INFO, WARNING, ERROR

# processors.pyì—ì„œ ì‚¬ìš©
import logging

logger = logging.getLogger(__name__)
logger.setLevel(getattr(logging, config.LOG_LEVEL))
```

### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```python
# profile_performance.py
import cProfile
import pstats

def profile_video_processor():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì½”ë“œ ì‹¤í–‰
    # ...
    
    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)
```

---

## ğŸ“ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

### MoveNet ì„¤ì¹˜
- [ ] ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [ ] ëª¨ë¸ í˜•ì‹ í™•ì¸ (TFLite)
- [ ] ì…ì¶œë ¥ shape í™•ì¸

### ì˜¤ë””ì˜¤ ëª¨ë¸ ì¤€ë¹„
- [ ] í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (ìµœì†Œ 1000ê°œ ìƒ˜í”Œ)
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- [ ] ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
- [ ] TFLite ë³€í™˜ ì™„ë£Œ
- [ ] config.pyì—ì„œ AUDIO_ENABLED = True

### ë°°í¬
- [ ] RPi 5ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
- [ ] ëª¨ë¸ íŒŒì¼ ë³µì‚¬
- [ ] main.py ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
